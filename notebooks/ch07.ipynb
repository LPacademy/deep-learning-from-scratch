{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ７章 畳み込みニューラルネットワーク（CNN）\n",
    "\n",
    "ニューラルネットワークの１つ  \n",
    "画像認識や音声認識で使われる  \n",
    "\n",
    "学習内容\n",
    "- 畳み込み、プーリング層を理解する\n",
    "- im2colを用いた効率の良い実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7.2 畳み込み層\n",
    "\n",
    "### 7.2.1\n",
    "全結合層（Affineレイヤ）では、画像の３次元情報（チャネル、縦、横）が保持されない。  \n",
    "*チャンネル: 色（白黒なら1, RGBなら3）。\n",
    "\n",
    "畳み込み層（covolutioレイヤ）ではこれらの情報が保持される。  \n",
    "＝畳み込み後も空間的な距離を保つ\n",
    "\n",
    "*特徴マップ: CNNでは入出力データのことを特にそういう。\n",
    "\n",
    "### 7.2.3\n",
    "畳み込み層を重ねると、どんどんデータのサイズが小さくなる。  \n",
    "パディングで入力特徴マップの周囲に0のデータを付け加えることで、出力特徴マップのサイズを調整できる。  \n",
    "\n",
    "### 7.2.4\n",
    "*ストライド: フィルターを適用する間隔\n",
    "\n",
    "### 7.2.5~7.2.7\n",
    "３次元でも同様"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 プーリング層\n",
    "空間サイズを小さくする処理。  \n",
    "フィルター内の最大値を取得するのが一般的。\n",
    "\n",
    "特徴\n",
    "- 学習パラメータなし\n",
    "- チャンネル数は変わらない\n",
    "- 位置ズレに対してロバスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 実装\n",
    "\n",
    "### 7.4.1\n",
    "データは４次元\n",
    "- データ数\n",
    "- チャンネル数\n",
    "- 高さ\n",
    "- 横幅\n",
    "\n",
    "### 7.4.2\n",
    "for文で実装すると複雑かつ遅くなるので、im2colを用いる。  \n",
    "一旦入力特徴マップとフィルターを行列に変換して、計算。  \n",
    "その後再び４次元に戻して出力とする。  \n",
    "最適化された線形代数ライブラリを使うことができる。\n",
    "\n",
    "### 7.4.3 Covolutionレイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "# print(x1)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 中間データ（backward時に使用）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 重み・バイアスパラメータの勾配\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"単純なConvNet\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"損失関数を求める\n",
    "        引数のxは入力データ、tは教師ラベル\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（数値微分）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.29906665969\n",
      "=== epoch:1, train acc:0.105, test acc:0.11 ===\n",
      "train loss:2.29380472319\n",
      "train loss:2.28949232021\n",
      "train loss:2.28806928092\n",
      "train loss:2.27073007993\n",
      "train loss:2.26666615989\n",
      "train loss:2.24339728801\n",
      "train loss:2.22747157261\n",
      "train loss:2.20490692703\n",
      "train loss:2.1749147401\n",
      "train loss:2.14640805414\n",
      "train loss:2.09453980234\n",
      "train loss:2.05864971962\n",
      "train loss:1.9762031688\n",
      "train loss:1.92219566313\n",
      "train loss:1.86534178705\n",
      "train loss:1.7892154691\n",
      "train loss:1.77573009086\n",
      "train loss:1.65556387824\n",
      "train loss:1.57560156446\n",
      "train loss:1.40372246484\n",
      "train loss:1.46545444819\n",
      "train loss:1.34032259269\n",
      "train loss:1.1504252287\n",
      "train loss:1.09787488776\n",
      "train loss:1.06099783577\n",
      "train loss:1.00086031225\n",
      "train loss:0.910254889604\n",
      "train loss:0.841556797006\n",
      "train loss:0.833852331062\n",
      "train loss:0.756192994892\n",
      "train loss:0.72734854017\n",
      "train loss:0.671570986121\n",
      "train loss:0.842522225525\n",
      "train loss:0.759788986754\n",
      "train loss:0.746287451202\n",
      "train loss:0.709724785875\n",
      "train loss:0.499173034714\n",
      "train loss:0.559227733833\n",
      "train loss:0.634119647761\n",
      "train loss:0.435453636413\n",
      "train loss:0.421822849476\n",
      "train loss:0.482803701561\n",
      "train loss:0.400357079709\n",
      "train loss:0.428084360696\n",
      "train loss:0.461931625767\n",
      "train loss:0.706825412587\n",
      "train loss:0.454279995718\n",
      "train loss:0.526497565507\n",
      "train loss:0.629573272653\n",
      "train loss:0.487673331099\n",
      "=== epoch:2, train acc:0.787, test acc:0.779 ===\n",
      "train loss:0.637140158832\n",
      "train loss:0.524580224359\n",
      "train loss:0.577359118819\n",
      "train loss:0.601783501554\n",
      "train loss:0.402541729529\n",
      "train loss:0.430648935573\n",
      "train loss:0.562994926639\n",
      "train loss:0.537412051585\n",
      "train loss:0.490610103304\n",
      "train loss:0.390044199616\n",
      "train loss:0.49028865451\n",
      "train loss:0.546652696678\n",
      "train loss:0.380940839267\n",
      "train loss:0.445478329601\n",
      "train loss:0.438656885129\n",
      "train loss:0.372903173689\n",
      "train loss:0.653967402473\n",
      "train loss:0.438536258894\n",
      "train loss:0.341243424498\n",
      "train loss:0.347964098996\n",
      "train loss:0.305986950848\n",
      "train loss:0.483049764052\n",
      "train loss:0.46396809832\n",
      "train loss:0.359294634163\n",
      "train loss:0.342072652473\n",
      "train loss:0.343533066897\n",
      "train loss:0.339658042253\n",
      "train loss:0.321190384795\n",
      "train loss:0.592361708466\n",
      "train loss:0.426099154713\n",
      "train loss:0.241694725556\n",
      "train loss:0.355760411412\n",
      "train loss:0.505229632176\n",
      "train loss:0.359639884028\n",
      "train loss:0.246618370596\n",
      "train loss:0.514442113008\n",
      "train loss:0.430638654505\n",
      "train loss:0.444324432336\n",
      "train loss:0.494284474182\n",
      "train loss:0.3852415672\n",
      "train loss:0.407948519949\n",
      "train loss:0.1987760962\n",
      "train loss:0.371419323892\n",
      "train loss:0.263548011637\n",
      "train loss:0.238875519849\n",
      "train loss:0.435443624214\n",
      "train loss:0.354327085073\n",
      "train loss:0.262173720882\n",
      "train loss:0.303046755984\n",
      "train loss:0.173509516612\n",
      "=== epoch:3, train acc:0.875, test acc:0.874 ===\n",
      "train loss:0.357087400039\n",
      "train loss:0.37788408071\n",
      "train loss:0.172105738322\n",
      "train loss:0.284982234283\n",
      "train loss:0.371963177165\n",
      "train loss:0.287169933489\n",
      "train loss:0.290077171823\n",
      "train loss:0.304255003602\n",
      "train loss:0.301899246001\n",
      "train loss:0.192115545596\n",
      "train loss:0.315521851723\n",
      "train loss:0.364184559568\n",
      "train loss:0.306016635912\n",
      "train loss:0.329871741429\n",
      "train loss:0.299519807818\n",
      "train loss:0.348446851252\n",
      "train loss:0.368615632665\n",
      "train loss:0.296097196393\n",
      "train loss:0.252487338249\n",
      "train loss:0.351650165331\n",
      "train loss:0.175342561219\n",
      "train loss:0.397828605054\n",
      "train loss:0.290949016663\n",
      "train loss:0.301563964162\n",
      "train loss:0.31522610598\n",
      "train loss:0.258692943799\n",
      "train loss:0.492068148403\n",
      "train loss:0.355369519195\n",
      "train loss:0.232021268662\n",
      "train loss:0.387278636241\n",
      "train loss:0.431961366056\n",
      "train loss:0.301357512858\n",
      "train loss:0.209482265198\n",
      "train loss:0.281573545397\n",
      "train loss:0.31015590123\n",
      "train loss:0.325604510312\n",
      "train loss:0.330096136669\n",
      "train loss:0.260511242573\n",
      "train loss:0.225466545508\n",
      "train loss:0.38977549947\n",
      "train loss:0.256011930217\n",
      "train loss:0.234927314667\n",
      "train loss:0.20588256849\n",
      "train loss:0.224878481795\n",
      "train loss:0.252429063047\n",
      "train loss:0.201342656059\n",
      "train loss:0.215726483386\n",
      "train loss:0.265622074991\n",
      "train loss:0.291756586346\n",
      "train loss:0.188261753173\n",
      "=== epoch:4, train acc:0.9, test acc:0.89 ===\n",
      "train loss:0.245994311528\n",
      "train loss:0.294047584777\n",
      "train loss:0.362627841625\n",
      "train loss:0.237011278049\n",
      "train loss:0.258555132134\n",
      "train loss:0.169457509129\n",
      "train loss:0.239801014768\n",
      "train loss:0.175980456087\n",
      "train loss:0.255996343005\n",
      "train loss:0.279991453457\n",
      "train loss:0.300803274867\n",
      "train loss:0.351471048278\n",
      "train loss:0.252414010766\n",
      "train loss:0.225222858002\n",
      "train loss:0.338100114206\n",
      "train loss:0.395124105319\n",
      "train loss:0.287972166848\n",
      "train loss:0.251456285958\n",
      "train loss:0.327274815526\n",
      "train loss:0.189414348762\n",
      "train loss:0.296011199774\n",
      "train loss:0.112755264791\n",
      "train loss:0.17429787713\n",
      "train loss:0.264878353291\n",
      "train loss:0.286528007718\n",
      "train loss:0.215146126435\n",
      "train loss:0.149181375625\n",
      "train loss:0.253901822105\n",
      "train loss:0.240499175269\n",
      "train loss:0.229960483884\n",
      "train loss:0.191524185831\n",
      "train loss:0.192623358365\n",
      "train loss:0.177748086076\n",
      "train loss:0.213447447606\n",
      "train loss:0.171780854145\n",
      "train loss:0.195882478442\n",
      "train loss:0.314016235038\n",
      "train loss:0.298426243254\n",
      "train loss:0.254940591081\n",
      "train loss:0.211748571104\n",
      "train loss:0.284486556569\n",
      "train loss:0.359772162953\n",
      "train loss:0.280889247803\n",
      "train loss:0.281780701582\n",
      "train loss:0.204535317454\n",
      "train loss:0.121265464825\n",
      "train loss:0.193836861565\n",
      "train loss:0.130721660686\n",
      "train loss:0.313046168069\n",
      "train loss:0.152301001537\n",
      "=== epoch:5, train acc:0.916, test acc:0.906 ===\n",
      "train loss:0.183888633687\n",
      "train loss:0.244916608987\n",
      "train loss:0.229112871327\n",
      "train loss:0.194494662943\n",
      "train loss:0.163809119212\n",
      "train loss:0.198436138145\n",
      "train loss:0.201883085183\n",
      "train loss:0.222647501701\n",
      "train loss:0.172122536306\n",
      "train loss:0.4291953781\n",
      "train loss:0.169318906565\n",
      "train loss:0.242673163904\n",
      "train loss:0.252346466236\n",
      "train loss:0.12168984343\n",
      "train loss:0.173568052195\n",
      "train loss:0.302485702176\n",
      "train loss:0.169648381692\n",
      "train loss:0.225091250212\n",
      "train loss:0.252891900979\n",
      "train loss:0.157977315893\n",
      "train loss:0.156790920555\n",
      "train loss:0.188340524726\n",
      "train loss:0.112578472086\n",
      "train loss:0.169987885259\n",
      "train loss:0.143176017639\n",
      "train loss:0.228118936406\n",
      "train loss:0.158218174106\n",
      "train loss:0.348323029961\n",
      "train loss:0.238169191405\n",
      "train loss:0.169340583275\n",
      "train loss:0.147179227169\n",
      "train loss:0.282419836137\n",
      "train loss:0.187347825895\n",
      "train loss:0.215893841992\n",
      "train loss:0.220996527822\n",
      "train loss:0.13336508343\n",
      "train loss:0.192480645403\n",
      "train loss:0.208021888625\n",
      "train loss:0.134479907994\n",
      "train loss:0.187624696501\n",
      "train loss:0.277075187239\n",
      "train loss:0.225366173637\n",
      "train loss:0.153069569516\n",
      "train loss:0.223029272881\n",
      "train loss:0.0956557433361\n",
      "train loss:0.240048660195\n",
      "train loss:0.139240591007\n",
      "train loss:0.160042931845\n",
      "train loss:0.173571300075\n",
      "train loss:0.17398696555\n",
      "=== epoch:6, train acc:0.92, test acc:0.92 ===\n",
      "train loss:0.120523480257\n",
      "train loss:0.237574502642\n",
      "train loss:0.174583589779\n",
      "train loss:0.21574707712\n",
      "train loss:0.136321389911\n",
      "train loss:0.173799078736\n",
      "train loss:0.155171040655\n",
      "train loss:0.293935260328\n",
      "train loss:0.181478590135\n",
      "train loss:0.348817309442\n",
      "train loss:0.18721720203\n",
      "train loss:0.330119681584\n",
      "train loss:0.0990394376333\n",
      "train loss:0.092225956298\n",
      "train loss:0.187060774835\n",
      "train loss:0.185361775724\n",
      "train loss:0.327543481176\n",
      "train loss:0.260231992492\n",
      "train loss:0.150677589086\n",
      "train loss:0.144771908495\n",
      "train loss:0.196662546128\n",
      "train loss:0.274349560492\n",
      "train loss:0.118073853857\n",
      "train loss:0.176569161733\n",
      "train loss:0.135611535696\n",
      "train loss:0.164999039004\n",
      "train loss:0.252510701228\n",
      "train loss:0.133664899916\n",
      "train loss:0.205209732918\n",
      "train loss:0.150900359241\n",
      "train loss:0.22235185043\n",
      "train loss:0.139670413886\n",
      "train loss:0.206295024147\n",
      "train loss:0.134104395753\n",
      "train loss:0.11540145161\n",
      "train loss:0.105320250992\n",
      "train loss:0.160241153326\n",
      "train loss:0.107633825956\n",
      "train loss:0.114241183431\n",
      "train loss:0.115602991564\n",
      "train loss:0.147556024823\n",
      "train loss:0.214280343313\n",
      "train loss:0.152404936728\n",
      "train loss:0.155542779097\n",
      "train loss:0.138794064491\n",
      "train loss:0.154533803602\n",
      "train loss:0.148408718996\n",
      "train loss:0.202419568433\n",
      "train loss:0.177728069614\n",
      "train loss:0.178392205974\n",
      "=== epoch:7, train acc:0.947, test acc:0.921 ===\n",
      "train loss:0.220466850291\n",
      "train loss:0.1334819869\n",
      "train loss:0.12939496792\n",
      "train loss:0.216363172876\n",
      "train loss:0.149278987473\n",
      "train loss:0.188979774258\n",
      "train loss:0.127051004309\n",
      "train loss:0.215967297029\n",
      "train loss:0.255021119287\n",
      "train loss:0.117018060228\n",
      "train loss:0.177922027421\n",
      "train loss:0.197205848557\n",
      "train loss:0.0953730785659\n",
      "train loss:0.193500324376\n",
      "train loss:0.135377832787\n",
      "train loss:0.0813409354692\n",
      "train loss:0.193983845281\n",
      "train loss:0.113839270304\n",
      "train loss:0.165015801822\n",
      "train loss:0.178592986196\n",
      "train loss:0.17586152085\n",
      "train loss:0.252533891982\n",
      "train loss:0.160129463536\n",
      "train loss:0.170645630157\n",
      "train loss:0.0759030364586\n",
      "train loss:0.152837872622\n",
      "train loss:0.146285106563\n",
      "train loss:0.0981866846429\n",
      "train loss:0.121066059145\n",
      "train loss:0.0843405695569\n",
      "train loss:0.0842291235108\n",
      "train loss:0.187560901006\n",
      "train loss:0.121101454576\n",
      "train loss:0.158796948679\n",
      "train loss:0.164495210383\n",
      "train loss:0.1074886758\n",
      "train loss:0.187007689379\n",
      "train loss:0.0740904387813\n",
      "train loss:0.130688556428\n",
      "train loss:0.142376179217\n",
      "train loss:0.142736691015\n",
      "train loss:0.144908257999\n",
      "train loss:0.0461135154686\n",
      "train loss:0.11669798237\n",
      "train loss:0.11832768874\n",
      "train loss:0.0690571600093\n",
      "train loss:0.105193304662\n",
      "train loss:0.187120568092\n",
      "train loss:0.114155068213\n",
      "train loss:0.13934739258\n",
      "=== epoch:8, train acc:0.938, test acc:0.929 ===\n",
      "train loss:0.224540367873\n",
      "train loss:0.0852755820483\n",
      "train loss:0.0623993721196\n",
      "train loss:0.0824904344884\n",
      "train loss:0.118742709956\n",
      "train loss:0.0747804400024\n",
      "train loss:0.0934715380319\n",
      "train loss:0.120044443091\n",
      "train loss:0.132992316311\n",
      "train loss:0.128427041294\n",
      "train loss:0.0819500051629\n",
      "train loss:0.0898885875633\n",
      "train loss:0.0677097993759\n",
      "train loss:0.0757733799957\n",
      "train loss:0.0808509020676\n",
      "train loss:0.133783210065\n",
      "train loss:0.124996823438\n",
      "train loss:0.103957304621\n",
      "train loss:0.102415232112\n",
      "train loss:0.106542829833\n",
      "train loss:0.051996601308\n",
      "train loss:0.0479287524841\n",
      "train loss:0.114522621559\n",
      "train loss:0.0894190269202\n",
      "train loss:0.258309273028\n",
      "train loss:0.129706265766\n",
      "train loss:0.0603873611921\n",
      "train loss:0.124496944967\n",
      "train loss:0.154173042888\n",
      "train loss:0.0681893483921\n",
      "train loss:0.15810938842\n",
      "train loss:0.216346003075\n",
      "train loss:0.0658548025452\n",
      "train loss:0.231933344455\n",
      "train loss:0.109758804598\n",
      "train loss:0.0920847796901\n",
      "train loss:0.103348532545\n",
      "train loss:0.209691305277\n",
      "train loss:0.127060828118\n",
      "train loss:0.0805354375915\n",
      "train loss:0.0730825285345\n",
      "train loss:0.10513879608\n",
      "train loss:0.0930710999798\n",
      "train loss:0.123128678131\n",
      "train loss:0.218362545917\n",
      "train loss:0.152853919515\n",
      "train loss:0.0831547716065\n",
      "train loss:0.12555741499\n",
      "train loss:0.142621464127\n",
      "train loss:0.141519090874\n",
      "=== epoch:9, train acc:0.956, test acc:0.936 ===\n",
      "train loss:0.0716873985967\n",
      "train loss:0.113324571148\n",
      "train loss:0.121473179271\n",
      "train loss:0.112175320322\n",
      "train loss:0.118382509673\n",
      "train loss:0.12404102604\n",
      "train loss:0.0902816827525\n",
      "train loss:0.137323156861\n",
      "train loss:0.0724151270213\n",
      "train loss:0.114476376206\n",
      "train loss:0.130489521698\n",
      "train loss:0.0968818267343\n",
      "train loss:0.0583227360166\n",
      "train loss:0.118827255483\n",
      "train loss:0.06708615252\n",
      "train loss:0.0913242938517\n",
      "train loss:0.102228163967\n",
      "train loss:0.0994607746372\n",
      "train loss:0.0519927877809\n",
      "train loss:0.111917851737\n",
      "train loss:0.0914306410278\n",
      "train loss:0.102449376587\n",
      "train loss:0.137718400878\n",
      "train loss:0.0915847131926\n",
      "train loss:0.111238875847\n",
      "train loss:0.111904211397\n",
      "train loss:0.0804100944169\n",
      "train loss:0.148772711663\n",
      "train loss:0.11417281375\n",
      "train loss:0.0793789464397\n",
      "train loss:0.141117556505\n",
      "train loss:0.117372658469\n",
      "train loss:0.147520896572\n",
      "train loss:0.0353422499545\n",
      "train loss:0.0775888509579\n",
      "train loss:0.151921896445\n",
      "train loss:0.146482855534\n",
      "train loss:0.187095340737\n",
      "train loss:0.120294242185\n",
      "train loss:0.0992577285145\n",
      "train loss:0.212239345143\n",
      "train loss:0.048681274457\n",
      "train loss:0.0908049060315\n",
      "train loss:0.0460493179234\n",
      "train loss:0.200961765178\n",
      "train loss:0.118460292375\n",
      "train loss:0.184983079334\n",
      "train loss:0.0643476179686\n",
      "train loss:0.0936667754399\n",
      "train loss:0.11675413309\n",
      "=== epoch:10, train acc:0.962, test acc:0.936 ===\n",
      "train loss:0.190580603993\n",
      "train loss:0.110063868837\n",
      "train loss:0.171530516838\n",
      "train loss:0.120387855987\n",
      "train loss:0.102337781821\n",
      "train loss:0.0728850278886\n",
      "train loss:0.0757079558301\n",
      "train loss:0.053508894498\n",
      "train loss:0.213552630556\n",
      "train loss:0.0886056128151\n",
      "train loss:0.124531757411\n",
      "train loss:0.148201534792\n",
      "train loss:0.101012119731\n",
      "train loss:0.144185073753\n",
      "train loss:0.168999558766\n",
      "train loss:0.0765174064535\n",
      "train loss:0.0689913117149\n",
      "train loss:0.116718175834\n",
      "train loss:0.0488346078406\n",
      "train loss:0.0638907959655\n",
      "train loss:0.20097751289\n",
      "train loss:0.118197857973\n",
      "train loss:0.0753886771089\n",
      "train loss:0.114326882702\n",
      "train loss:0.0751468458145\n",
      "train loss:0.0347439870785\n",
      "train loss:0.0936663231069\n",
      "train loss:0.0930867744825\n",
      "train loss:0.0645812955855\n",
      "train loss:0.0618271182138\n",
      "train loss:0.0453262963994\n",
      "train loss:0.100558372138\n",
      "train loss:0.127569401256\n",
      "train loss:0.0532993758834\n",
      "train loss:0.0660905626785\n",
      "train loss:0.0816565139523\n",
      "train loss:0.172720270637\n",
      "train loss:0.055910817169\n",
      "train loss:0.0676686553903\n",
      "train loss:0.0917848380929\n",
      "train loss:0.0938517014515\n",
      "train loss:0.11092460292\n",
      "train loss:0.112713182573\n",
      "train loss:0.0525248660013\n",
      "train loss:0.0763677796655\n",
      "train loss:0.0380463250254\n",
      "train loss:0.110235283504\n",
      "train loss:0.107879930835\n",
      "train loss:0.146979555924\n",
      "train loss:0.0887482780124\n",
      "=== epoch:11, train acc:0.965, test acc:0.95 ===\n",
      "train loss:0.027078034363\n",
      "train loss:0.17849695575\n",
      "train loss:0.0335730059743\n",
      "train loss:0.0489289777991\n",
      "train loss:0.0919017356119\n",
      "train loss:0.0633221300678\n",
      "train loss:0.0921298122042\n",
      "train loss:0.080746114658\n",
      "train loss:0.16498760578\n",
      "train loss:0.0514967383514\n",
      "train loss:0.0969389285649\n",
      "train loss:0.0512472960762\n",
      "train loss:0.0467290546215\n",
      "train loss:0.134110230065\n",
      "train loss:0.122395552894\n",
      "train loss:0.0740445952259\n",
      "train loss:0.0984097939633\n",
      "train loss:0.106059860003\n",
      "train loss:0.0848472811702\n",
      "train loss:0.178458772625\n",
      "train loss:0.072369849964\n",
      "train loss:0.071190387391\n",
      "train loss:0.102639727041\n",
      "train loss:0.0633760083513\n",
      "train loss:0.139051139759\n",
      "train loss:0.0589998061107\n",
      "train loss:0.0971135834342\n",
      "train loss:0.0497778657295\n",
      "train loss:0.0703322921763\n",
      "train loss:0.038132835694\n",
      "train loss:0.0420917955265\n",
      "train loss:0.0529916964701\n",
      "train loss:0.130829816337\n",
      "train loss:0.189566450619\n",
      "train loss:0.0327636289107\n",
      "train loss:0.109302451782\n",
      "train loss:0.0444194593359\n",
      "train loss:0.0902837919637\n",
      "train loss:0.101157904116\n",
      "train loss:0.08463141127\n",
      "train loss:0.026449012087\n",
      "train loss:0.103078402207\n",
      "train loss:0.0688813467924\n",
      "train loss:0.0523863589674\n",
      "train loss:0.108259776935\n",
      "train loss:0.0490028110939\n",
      "train loss:0.073446108167\n",
      "train loss:0.0950800831499\n",
      "train loss:0.0689608893978\n",
      "train loss:0.0946454470617\n",
      "=== epoch:12, train acc:0.966, test acc:0.95 ===\n",
      "train loss:0.0582359585612\n",
      "train loss:0.131396140847\n",
      "train loss:0.0617084250931\n",
      "train loss:0.0556988403735\n",
      "train loss:0.0781364016653\n",
      "train loss:0.0371394397543\n",
      "train loss:0.0364040671495\n",
      "train loss:0.0547348637504\n",
      "train loss:0.0376141501202\n",
      "train loss:0.102303530836\n",
      "train loss:0.031392473764\n",
      "train loss:0.070945166526\n",
      "train loss:0.0944288978104\n",
      "train loss:0.0512725634\n",
      "train loss:0.0765358800303\n",
      "train loss:0.0192714111965\n",
      "train loss:0.0596562840888\n",
      "train loss:0.078471886024\n",
      "train loss:0.0571166943188\n",
      "train loss:0.0555962187921\n",
      "train loss:0.10117229502\n",
      "train loss:0.0821388089697\n",
      "train loss:0.0934315164052\n",
      "train loss:0.149013515408\n",
      "train loss:0.0633378987302\n",
      "train loss:0.0482768438153\n",
      "train loss:0.065444815356\n",
      "train loss:0.0562765084615\n",
      "train loss:0.0651738911182\n",
      "train loss:0.0373940612452\n",
      "train loss:0.0537157343984\n",
      "train loss:0.0394210846814\n",
      "train loss:0.107601016626\n",
      "train loss:0.109068293031\n",
      "train loss:0.0538006282438\n",
      "train loss:0.0503247371464\n",
      "train loss:0.0556711474817\n",
      "train loss:0.0707916571945\n",
      "train loss:0.108410621221\n",
      "train loss:0.0467629130741\n",
      "train loss:0.0245349542067\n",
      "train loss:0.0590709259742\n",
      "train loss:0.0495995259352\n",
      "train loss:0.0538874976388\n",
      "train loss:0.0674893851429\n",
      "train loss:0.0650676122455\n",
      "train loss:0.0229592411354\n",
      "train loss:0.0403832257645\n",
      "train loss:0.0478076931807\n",
      "train loss:0.0401396953428\n",
      "=== epoch:13, train acc:0.973, test acc:0.954 ===\n",
      "train loss:0.0643199822637\n",
      "train loss:0.0358995984195\n",
      "train loss:0.0714587706711\n",
      "train loss:0.0369015581858\n",
      "train loss:0.0955190723948\n",
      "train loss:0.062273716994\n",
      "train loss:0.0678748153755\n",
      "train loss:0.0457178417677\n",
      "train loss:0.112955530494\n",
      "train loss:0.0780342548772\n",
      "train loss:0.110234845506\n",
      "train loss:0.150129224375\n",
      "train loss:0.0471967057004\n",
      "train loss:0.0475086228614\n",
      "train loss:0.107921401503\n",
      "train loss:0.0484391540279\n",
      "train loss:0.0776356922132\n",
      "train loss:0.095626005298\n",
      "train loss:0.0328351014794\n",
      "train loss:0.0625112164017\n",
      "train loss:0.0807017662084\n",
      "train loss:0.065189674882\n",
      "train loss:0.0346313255858\n",
      "train loss:0.0845651459326\n",
      "train loss:0.0594883172708\n",
      "train loss:0.0249027274333\n",
      "train loss:0.154677559751\n",
      "train loss:0.106652622859\n",
      "train loss:0.0629440903645\n",
      "train loss:0.0602112211671\n",
      "train loss:0.0655947499748\n",
      "train loss:0.0491468835007\n",
      "train loss:0.0231343405943\n",
      "train loss:0.0542274159948\n",
      "train loss:0.0199395837392\n",
      "train loss:0.0393536553483\n",
      "train loss:0.0380424777236\n",
      "train loss:0.0356549047423\n",
      "train loss:0.0331869294939\n",
      "train loss:0.0424011392267\n",
      "train loss:0.0501590382125\n",
      "train loss:0.0366315973081\n",
      "train loss:0.0290412951391\n",
      "train loss:0.0931378766799\n",
      "train loss:0.047926184997\n",
      "train loss:0.055841647189\n",
      "train loss:0.0471551211539\n",
      "train loss:0.0601892637808\n",
      "train loss:0.0252467146833\n",
      "train loss:0.0273604474678\n",
      "=== epoch:14, train acc:0.98, test acc:0.954 ===\n",
      "train loss:0.0959857061998\n",
      "train loss:0.0729730606435\n",
      "train loss:0.0250558750018\n",
      "train loss:0.0608298641592\n",
      "train loss:0.084315426744\n",
      "train loss:0.0389974796131\n",
      "train loss:0.04344164507\n",
      "train loss:0.0318620240721\n",
      "train loss:0.100461597243\n",
      "train loss:0.0958610969977\n",
      "train loss:0.0349731268243\n",
      "train loss:0.0466396059591\n",
      "train loss:0.0511400732359\n",
      "train loss:0.0464246969332\n",
      "train loss:0.0444417765626\n",
      "train loss:0.112387953504\n",
      "train loss:0.0449706131981\n",
      "train loss:0.0365498139017\n",
      "train loss:0.0743566082525\n",
      "train loss:0.046093062734\n",
      "train loss:0.0283783912439\n",
      "train loss:0.0395788430757\n",
      "train loss:0.0366642202418\n",
      "train loss:0.0172934222168\n",
      "train loss:0.0790643182956\n",
      "train loss:0.0192708645205\n",
      "train loss:0.0346173093904\n",
      "train loss:0.0483305803024\n",
      "train loss:0.0420049026974\n",
      "train loss:0.0443815054633\n",
      "train loss:0.0295841159076\n",
      "train loss:0.0372884981551\n",
      "train loss:0.026074783\n",
      "train loss:0.0377661711939\n",
      "train loss:0.0682011751728\n",
      "train loss:0.021541195285\n",
      "train loss:0.0473820093859\n",
      "train loss:0.0741156192269\n",
      "train loss:0.0887604895748\n",
      "train loss:0.0376704579323\n",
      "train loss:0.0426168894578\n",
      "train loss:0.0624289498653\n",
      "train loss:0.0525021695049\n",
      "train loss:0.0220142275346\n",
      "train loss:0.017244060702\n",
      "train loss:0.0405413648072\n",
      "train loss:0.024027602988\n",
      "train loss:0.0224259669798\n",
      "train loss:0.0387928558137\n",
      "train loss:0.0266586640396\n",
      "=== epoch:15, train acc:0.984, test acc:0.95 ===\n",
      "train loss:0.0243888401365\n",
      "train loss:0.0284398967814\n",
      "train loss:0.0699346750907\n",
      "train loss:0.051924547588\n",
      "train loss:0.0388810067057\n",
      "train loss:0.0217627761677\n",
      "train loss:0.0562466215886\n",
      "train loss:0.0192827060228\n",
      "train loss:0.0427788140281\n",
      "train loss:0.10975456332\n",
      "train loss:0.0316645977645\n",
      "train loss:0.0525562712006\n",
      "train loss:0.0405266970755\n",
      "train loss:0.0324363263921\n",
      "train loss:0.0676084279417\n",
      "train loss:0.0310097516146\n",
      "train loss:0.0651386206131\n",
      "train loss:0.0190370427153\n",
      "train loss:0.041341257692\n",
      "train loss:0.0219101833814\n",
      "train loss:0.0230688856891\n",
      "train loss:0.0422127571252\n",
      "train loss:0.0248302894165\n",
      "train loss:0.0197830451718\n",
      "train loss:0.0430177545137\n",
      "train loss:0.0104058881794\n",
      "train loss:0.0424477881897\n",
      "train loss:0.0225705103478\n",
      "train loss:0.0486700536911\n",
      "train loss:0.0228154282497\n",
      "train loss:0.066943796133\n",
      "train loss:0.0141670040231\n",
      "train loss:0.0300894096087\n",
      "train loss:0.0764739270928\n",
      "train loss:0.0251100166452\n",
      "train loss:0.0648135550053\n",
      "train loss:0.0250264275159\n",
      "train loss:0.0528715580576\n",
      "train loss:0.0190917789272\n",
      "train loss:0.0290692024874\n",
      "train loss:0.055531316423\n",
      "train loss:0.0232941679383\n",
      "train loss:0.049212338566\n",
      "train loss:0.0448570034747\n",
      "train loss:0.0234094047606\n",
      "train loss:0.0666854695201\n",
      "train loss:0.0420446351053\n",
      "train loss:0.0861310009248\n",
      "train loss:0.0254715304452\n",
      "train loss:0.0440269754796\n",
      "=== epoch:16, train acc:0.979, test acc:0.946 ===\n",
      "train loss:0.0718000714531\n",
      "train loss:0.0612873225053\n",
      "train loss:0.0994167917351\n",
      "train loss:0.0307219951103\n",
      "train loss:0.0113669495383\n",
      "train loss:0.0159511602121\n",
      "train loss:0.0270976193646\n",
      "train loss:0.03142128082\n",
      "train loss:0.010055625167\n",
      "train loss:0.071698890157\n",
      "train loss:0.0501041995045\n",
      "train loss:0.0267036401823\n",
      "train loss:0.0290690259075\n",
      "train loss:0.0495582893529\n",
      "train loss:0.0442990879101\n",
      "train loss:0.0254718250803\n",
      "train loss:0.0251604519813\n",
      "train loss:0.0113933969831\n",
      "train loss:0.102710216258\n",
      "train loss:0.0342626328879\n",
      "train loss:0.0384290911429\n",
      "train loss:0.0283796901242\n",
      "train loss:0.0432112830526\n",
      "train loss:0.0486930979989\n",
      "train loss:0.0480281747247\n",
      "train loss:0.0216254427061\n",
      "train loss:0.0259162971926\n",
      "train loss:0.0177672121434\n",
      "train loss:0.0127980441362\n",
      "train loss:0.0319933580324\n",
      "train loss:0.0414673413689\n",
      "train loss:0.0299709117818\n",
      "train loss:0.0192445077327\n",
      "train loss:0.0174433771495\n",
      "train loss:0.041563415459\n",
      "train loss:0.0256339280636\n",
      "train loss:0.0295694733939\n",
      "train loss:0.0274075349632\n",
      "train loss:0.0290853873302\n",
      "train loss:0.0413781660398\n",
      "train loss:0.0208676050075\n",
      "train loss:0.0246644432997\n",
      "train loss:0.0210069591865\n",
      "train loss:0.0373421964241\n",
      "train loss:0.0158371214996\n",
      "train loss:0.0283256008841\n",
      "train loss:0.0263213540793\n",
      "train loss:0.0286040819333\n",
      "train loss:0.041436130105\n",
      "train loss:0.0284387986903\n",
      "=== epoch:17, train acc:0.985, test acc:0.958 ===\n",
      "train loss:0.0439016033255\n",
      "train loss:0.0277438660726\n",
      "train loss:0.0934010275349\n",
      "train loss:0.0215626906583\n",
      "train loss:0.0243143059689\n",
      "train loss:0.0240548994195\n",
      "train loss:0.0456758125069\n",
      "train loss:0.025311808328\n",
      "train loss:0.00490963741155\n",
      "train loss:0.00800987850123\n",
      "train loss:0.0153321819968\n",
      "train loss:0.110341725806\n",
      "train loss:0.0118910929724\n",
      "train loss:0.00947230471868\n",
      "train loss:0.0456515915641\n",
      "train loss:0.0526643728127\n",
      "train loss:0.0432727376446\n",
      "train loss:0.0257729414519\n",
      "train loss:0.0222902157462\n",
      "train loss:0.0299941153104\n",
      "train loss:0.0456107152444\n",
      "train loss:0.0162300964659\n",
      "train loss:0.0268837861592\n",
      "train loss:0.0267209567859\n",
      "train loss:0.0303719247128\n",
      "train loss:0.0246748102526\n",
      "train loss:0.0503982239146\n",
      "train loss:0.0139882091112\n",
      "train loss:0.0219795032633\n",
      "train loss:0.00418073719637\n",
      "train loss:0.010495949823\n",
      "train loss:0.0263987812526\n",
      "train loss:0.0343182702659\n",
      "train loss:0.0143606712064\n",
      "train loss:0.0347932566178\n",
      "train loss:0.0254916300618\n",
      "train loss:0.0365015653536\n",
      "train loss:0.0384906210805\n",
      "train loss:0.0446255213151\n",
      "train loss:0.0307493780151\n",
      "train loss:0.0202997707876\n",
      "train loss:0.0260649525041\n",
      "train loss:0.0103013048769\n",
      "train loss:0.0312069929243\n",
      "train loss:0.0325340994399\n",
      "train loss:0.0249311648253\n",
      "train loss:0.0172990670234\n",
      "train loss:0.0567564018885\n",
      "train loss:0.0196536652406\n",
      "train loss:0.0083559181739\n",
      "=== epoch:18, train acc:0.988, test acc:0.95 ===\n",
      "train loss:0.0142353532459\n",
      "train loss:0.0311078618255\n",
      "train loss:0.0339400233458\n",
      "train loss:0.0368886558759\n",
      "train loss:0.0257325706618\n",
      "train loss:0.0123586718722\n",
      "train loss:0.0204335561781\n",
      "train loss:0.0366742195985\n",
      "train loss:0.0242958123354\n",
      "train loss:0.0105180392277\n",
      "train loss:0.0261566749875\n",
      "train loss:0.0149185956792\n",
      "train loss:0.0146543160836\n",
      "train loss:0.00687811559433\n",
      "train loss:0.0136682039021\n",
      "train loss:0.00929665175547\n",
      "train loss:0.0223394727605\n",
      "train loss:0.0088802036822\n",
      "train loss:0.0691203368013\n",
      "train loss:0.0244943542241\n",
      "train loss:0.0230286071002\n",
      "train loss:0.00913082333786\n",
      "train loss:0.0306791391932\n",
      "train loss:0.0246558108653\n",
      "train loss:0.0151309444796\n",
      "train loss:0.0246200371499\n",
      "train loss:0.00970591806699\n",
      "train loss:0.00720868859173\n",
      "train loss:0.0139342978095\n",
      "train loss:0.0138240978942\n",
      "train loss:0.0117929823714\n",
      "train loss:0.0533214453013\n",
      "train loss:0.0153561465315\n",
      "train loss:0.0118751819986\n",
      "train loss:0.0110763845554\n",
      "train loss:0.00829836527336\n",
      "train loss:0.0188999444995\n",
      "train loss:0.0101215428878\n",
      "train loss:0.0323968757459\n",
      "train loss:0.0203245502706\n",
      "train loss:0.13366444694\n",
      "train loss:0.0164488822385\n",
      "train loss:0.0407220750982\n",
      "train loss:0.0529026921845\n",
      "train loss:0.0259871944247\n",
      "train loss:0.156414070457\n",
      "train loss:0.0555821451743\n",
      "train loss:0.0528552289659\n",
      "train loss:0.0109691346157\n",
      "train loss:0.0229386239975\n",
      "=== epoch:19, train acc:0.983, test acc:0.955 ===\n",
      "train loss:0.0545540683717\n",
      "train loss:0.0209213178411\n",
      "train loss:0.00899972105959\n",
      "train loss:0.00874479583269\n",
      "train loss:0.0394137444489\n",
      "train loss:0.0515060745867\n",
      "train loss:0.0124384919979\n",
      "train loss:0.0303577075069\n",
      "train loss:0.0196995524075\n",
      "train loss:0.0270678927402\n",
      "train loss:0.0231880545413\n",
      "train loss:0.0127199534538\n",
      "train loss:0.036884142013\n",
      "train loss:0.0809556819638\n",
      "train loss:0.0223665461846\n",
      "train loss:0.0119193572713\n",
      "train loss:0.00907752349294\n",
      "train loss:0.0169218155399\n",
      "train loss:0.0764359003448\n",
      "train loss:0.0260269214097\n",
      "train loss:0.0208495047415\n",
      "train loss:0.0306755924517\n",
      "train loss:0.0237143349043\n",
      "train loss:0.010255132178\n",
      "train loss:0.0225008712964\n",
      "train loss:0.0177984229759\n",
      "train loss:0.0167667134905\n",
      "train loss:0.0146855573693\n",
      "train loss:0.0807220839386\n",
      "train loss:0.0252235136467\n",
      "train loss:0.0373009756368\n",
      "train loss:0.0300054989163\n",
      "train loss:0.0138613364273\n",
      "train loss:0.0254897837565\n",
      "train loss:0.0138035974073\n",
      "train loss:0.0285938067393\n",
      "train loss:0.0418046553693\n",
      "train loss:0.0206052638583\n",
      "train loss:0.0182529052054\n",
      "train loss:0.00638248803359\n",
      "train loss:0.00642888616136\n",
      "train loss:0.0218160168418\n",
      "train loss:0.0150910412562\n",
      "train loss:0.025480338326\n",
      "train loss:0.0187007012059\n",
      "train loss:0.0482117853928\n",
      "train loss:0.0242661940971\n",
      "train loss:0.0441740168468\n",
      "train loss:0.0127383043645\n",
      "train loss:0.0170136198457\n",
      "=== epoch:20, train acc:0.997, test acc:0.963 ===\n",
      "train loss:0.0122505537556\n",
      "train loss:0.0405829647191\n",
      "train loss:0.0109383254708\n",
      "train loss:0.0192269306973\n",
      "train loss:0.0378519836026\n",
      "train loss:0.0277649057813\n",
      "train loss:0.0122344913455\n",
      "train loss:0.0186031887263\n",
      "train loss:0.01718421657\n",
      "train loss:0.0149709221907\n",
      "train loss:0.031540518422\n",
      "train loss:0.0237102607849\n",
      "train loss:0.0122457616272\n",
      "train loss:0.0158906939836\n",
      "train loss:0.0116245812971\n",
      "train loss:0.0139294854267\n",
      "train loss:0.0239598717915\n",
      "train loss:0.0172935178068\n",
      "train loss:0.0087969278306\n",
      "train loss:0.0134134766983\n",
      "train loss:0.0578980052727\n",
      "train loss:0.0285767858527\n",
      "train loss:0.0173893977086\n",
      "train loss:0.0107667522476\n",
      "train loss:0.00889439796047\n",
      "train loss:0.014761243215\n",
      "train loss:0.0217210776127\n",
      "train loss:0.0082840806726\n",
      "train loss:0.0139990519741\n",
      "train loss:0.050916979632\n",
      "train loss:0.0555289120595\n",
      "train loss:0.0104864354697\n",
      "train loss:0.00520861926719\n",
      "train loss:0.00695303537244\n",
      "train loss:0.0593664748032\n",
      "train loss:0.0160340614689\n",
      "train loss:0.0201036097389\n",
      "train loss:0.0197891780993\n",
      "train loss:0.0143772982347\n",
      "train loss:0.0202780052376\n",
      "train loss:0.00659922259888\n",
      "train loss:0.0275847437946\n",
      "train loss:0.0160324962733\n",
      "train loss:0.00897619601147\n",
      "train loss:0.017082161713\n",
      "train loss:0.0176148541466\n",
      "train loss:0.0192496692906\n",
      "train loss:0.00827448151956\n",
      "train loss:0.0360046973\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.962\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEPCAYAAABGP2P1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VPW9//HXJytkg4CQSAIJIqhVAbfgTkRtUeu11bpU\n1Iu1Yr2V9tbeCrZFVOpt9dJeW7tYW38uVy22tla0vUWrjctVASsgKMhmwkxIAiaBbIRs398fM4Es\nE5hJZjKZ5P18POYxMydnznwyhO97zvd8v+eYcw4REZF2cdEuQEREBhYFg4iIdKJgEBGRThQMIiLS\niYJBREQ6UTCIiEgnEQ0GM3vUzCrM7INDrPMzM9tiZmvNbHok6xERkcOL9B7DY8DnevqhmV0ETHLO\nTQZuAR6OcD0iInIYEQ0G59xbQPUhVrkMeNK/7kpghJllRbImERE5tGgfY8gBPB2el/qXiYhIlEQ7\nGEREZIBJiPL7lwLjOzzP9S/rxsx0UicRkV5wzlko6/fHHoP5b4EsB24AMLPTgT3OuYqeNuSc0y1M\nt8WLF0e9hsF0G+qf5/btxUya9G2gDnBAHZMmfZvt24v77bPsbQ3NzY69ex1lZY5t2xwffOBYudJ3\n7/E46uocbW3hqWPvXsebbzoeesjx1a86Tj3VkZLimDzZceWVjvvuc/zlL47S0tDe81C33ojoHoOZ\nPQMUAqPNbAewGEgCnHPuEefcX83sYjPbCtQDN0ayHhEJj9ZWqK2Fmhrf/e23P862bfcAqf41Utm2\n7R7uuGMpy5YtJj4+/DXs2weffuq77d4N99wTuIZzzlnKsccupqEBGhqgvp4DjxsaoKUFUlMhJeXg\nbfhwaGyEqiqorvatk5kZ3O0Xvwhcx9SpS2lrW8wJJ8D06XDyyXDjjXDiiZCeHv7Ppy8iGgzOuWuD\nWOe2SNYgMth98kkJixY9TmlpGzk5cSxZMpeJE/MO+7qGBqiogPLyg7c9e3yNfXuD39PjffsgLQ0y\nMnw3j6eNgw1hu1Sef76N5GTfusE2rCUl8Oc/+xr79kY/0H1zM4wZ47sdcQRs3Rq4hlGj2li4sHPD\n3zEIkpLADtPRsn+/LyAC3aqqYMcOWLfO93zVqsB1fOYzbfzf/0FCtDvwgxADJUokFBYWRruEQaG9\nUd6w4RO2br0n6EY5nO9/4YUPdfiGWs9bby3moYfmEx+f16nR73irqPA1dtnZB29ZWTBqlK+hz8k5\n2Oinp3d/nJICcR06oq+7Lo6nn66nc4NYzzXXxPHEE75A6diQdm1cP/nk4OOKikKqqg42+Dk5MG3a\nweft92lpnRv0nmqYOjWOCy7o2+ecnHzwczqcnuqYPDkuJkIBwHrbB9XfzMzFSq0yNARqlCdNWswr\nr8wPazi0tfka00CN+3PP3UNx8X/QtRHKzFzKjBmLOzX8XUNgxIjDf1MOVn99FgO9hoFURzszw4V4\n8FnBIDGpt90n4TRnzj0880z3RvnSS5fywAOLaW319U0f6r79cWNj926d9ue7dvm+qbc36B0b+Mce\nW8ymTfd0q+288xbz2mvdl0dS+7/Jzp1tjBvX//8m8xbMY13xBxQXl7N/vyM52cjPz2Za/lQeuf+R\nfqsDov9ZdKRgkCGht9/I2tp8XRVd+6pra7sfkAx0kLLr88bGxUD3xnfYsMXk5d1DfLyvPzk+nk6P\nAy1r76ro2vBnZ8PYsb6fBzK54BS27koBOh7dbeXosQ1sWfXPPnzKsadwbiGvT3y92/KZn8yk6PGi\nfqtj3oJ5bK7Y3G35lKwpPQbUvuZ9lNaWUlpTSmltKd4aL6U1pYxJHcP3z/1+n+rpTTDESI+XDCTR\n/ra+aFHgUR/XXbeUq65a3OPByqoq3zfvrn3VI0b4+szT030Nc8cDk11Hq3R8fuK5L7Ft92t0bZRz\nxzawaVX/fFsfNTERLnmr+/KPZvTL+7frTWMYTm2ujX3N+wL+rL6pnpr9NWQkZ0S8DoDNFZsDBlTl\nh5X85p+/OdjwdwiC+qZ6xqWPIycjh9yMXHLSc8gfmc/xY4/vl5q7UjBISAJ9W3/33f7pP92/H957\nD955J/Coj61b29i61dfgT53aPQBGj4bExPDVM/qoRLZ9PnKNcnNrM7VNtdTsr6F2v+++/da+vGJ/\necDXFtd9wg3P3xDyew5LGEZGcgYZyRmkJ6UffJyc3m15enI6CXG+JqSnxpBPQi7hkJpam9hSuYWN\nn25k4+6NvvtPN7K5cjMt5S0wpftrNuzewLgfj8PMyEn3N7wZOeSk53R7PjZ1LPFx3cfWtrk2avfX\nUrWviurGaqr3VQe+b6xmXfk6mNi9jtLaUt71vktORg4FOQW+9/cHwejho7FwHfAJAwWDhKSnb+uL\nFi3lqacWh/W96urgnXfgzTfhjTd8oTBlCiQmxuGb9tK5b//CC+N46KGwlhBQc2szH1d+TE3r3oA/\n/3jvJs5/8vyQttnS1kLt/toDDX7N/hqaW5s7NcjdGuuk9B4nMGUOz+SCo0IbiuOco7Gl8UDwlOwt\n6RRCHQOqtqmW2v21JMUnkZGcwV7v3oCN4fbq7dz3xn2HDJiM5AxSk1KJs4PDnGr317Lp003dAqBk\nTwl5I/M47ojjOO6I45h99Gy+dfq3OPaIY7l086W8TvdwmpEzg3/c+Q/27t974Bt6aY3vW/v6Xev5\n27a/Hei62dO4h+y0bMalj6PVtR5o8Pc27iUlMYXM4ZlkDss8eD8sk1HDR5E5PJMJIyaQOTyTjSM2\nsoc93eqYmjWVRy97NKR/k2hRMEjQ9u2DtWsDf1t/9dU27r0XJkyAvDzfLTfXN0Y8WJWV8NZbvhB4\n80348EM46SQ491xYsADOPNPX7fPlWzdRYsfSuC8fXzdOK8OGF9OafnbYftd2exv38kHFB6wtX8va\n8rWsq1jHR7s/YvyI8VTtqwr4mrwReXz37O+G9D5xFtet8RyeMPyw3yLXLFvDDnZ0W56dls0N00Lf\nYwiFc46G5gZqm2r5lzX/wmpWd1snIS6B+uZ6yuvKqWmq6bb30x46Dc0NpCamkpGcgcOxp3EPU0ZP\nORAA1029juOOOI6jRx1NckIPB1wOwcwYOWwkI4eNPGT3zP6W/ZTVlVFaU0pifOKBEBg5bOSBvaPD\neXh47F89QMEgh9TWBq+/Dk89Bc8/D8nJgb+tT5gQR1MTvPqqb4LSjh2wc6evGycvr3NgtD9OS4N3\n3z0YBDt2wBlnwDnnwNKlcNppvhmoXZXtK6PxGi/gPbCsESj7pKzXv6dzDk+N50AAtIdARV0FJ2ad\nyLSsaZyWcxo3n3IzJ449kdSkVArfKwz4DTVzeCbnHxXaHkMsMjNSk1JJTUolJTEl4DoTRkzgP8//\nz8Nuq7Wtlfpm37GANtdGbkZupz2IYEzJmhKw62pKVoD+pR4kJySTPzKf/JH5Ib33YKNgkIDWr/eF\nwTPP+Prnr7sO7r0XmprmcuGFi7uNCFq2bD4Tu3QltLT4wmHHDl9YlJTABx/Aiy/6Hu/dCwUFviC4\n6SbfaQICTQBqbWtld8NuyuvKD9wC2fTpJm564aaQfk+Ho3hPMWvL1zIsYRjTs6czPXs6Vx9/NT88\n/4ccPerogH3OA0U4GsOBID4u/sAeU2/195DUngyGfxMFgxxQWgq/+50vECorYc4c+N//hRNOOLjO\nvAX3MfqUD6gdffyBseKj87P54cM13f5jJiT49g4mTICzu/TyOOfrLmhv6LfUlfPman/DX1/eKQSq\n9lUxavgostOyyUrNonpf4Gs/jUgewZnjzwz59/7yCV9mWvY0xqaODel1A6EBUGM48AyUf5O+0DyG\nGBKJYaI1NfCnP/nC4P334fLLfXsH557b+ZQH7YIZK97S1kJ5XfmBA3pdh+e1P06KT+LItCPJTss+\ncMtKzer0PDstmzGpYzr17w6U8eoisUDzGAaxcA4TrW9o5a9/r2fZs6288loLZ57VypU3tfCLp1tJ\nTGqlpa2Fjyt9962ulda2g4/3NHYfbQGwYdcGCn5TgLfGy6cNn3JEyhGdhgHmZuRy4tgTO43TTk3q\nehBbRAYCBUOM6GmY6A03LGXu3MU9ng2zqr6G3fEfsCd5LXVp62jKXIsb8yFxFk/yMfEMn5rAmvh4\n1pclcN8z8STEJRAfF0+8HXycEJdAvMUTHxfPJ9WBB6ZnpWbxs4t+Rm5GLtlp2UGP4OgNdVuIRJa6\nkgaw1lbYuBFWroT5d53MvsTuB+aS99dw7UXvk5buIMNLbcpaKhPXUs5avC3rqG4u4+iMEzjhiOlM\nP3IaBROmc0ruiWQM690J4NWNIxJb1JUU40pLfSGwapXv/p//9J2iYcYMiB9VBV9a0+01ca+kUzxz\nFusq1pEYl3hgVM212VcyPfs+Jo+aPKBH1YjIwKNgiJLaWt9M3vYQWLnSd8qHGTN8twULfOP4M0e1\nsaVyC+/cFE9dgO2MHjmSBWctYFr2NLLTgjhZfB+pG0dk8FNXUj9pbISXX/aN4X/nHd+FSaZN84VA\nQYHvfuJE2N2wi5XelawqXcXK0pWs3rmakcNGUv9yPbsLdnfbrrpwRORQ1JU0wLSHwR/+AC+95Dux\n2+WXw9e+5rvOa4s18H7Z+6wqXcWd769k5fKV7N2/l4KcAgrGFfCNGd/gtHGnkZWWReGawLNsRUTC\nTcEQZoHC4Kqr4Ef3t7E3cRMrvSv5TelKVr23io8rP+b4McdTkFPA5yd/nnsL72Xy6MkhnwpARCSc\nFAxhsH8/rFjROQw+96Uyzvu3lWypX8mfdq7iu0++x5iUMczIncGMnBnMnT6X6dnTGZYwLKj3UN++\niPQXHWMIQqBLBublZZMZN5WxiY/w4oo68s74JxPPXklL1irWVa6kobmBGTkzKMgpOHA/OmV0VOoX\nkaFLl/aMkBlXn86qz6zstjzhb2MYe0k21baNadlTKRhXcGCP4KjMowbUhTdEZGjSwecIKS4uh890\nXz4sroU/3/go07KnkRQfwoUHREQGMAVDEBobA++pxNenc1rOaf1cjYhIZGn4y2G0tUG91Qf8WXKy\nuopEZPBRMBzGV767htaMwGcUzc+P/ExjEZH+pq6kQ3jw15U83XIF5xw1k/0f1XcalZSfn820/KnR\nLlFEJOw0KqkHL/+9lUt/dzHXffZEHr16ab+9r4hIOGm4aphs3AinLvgex5z/DqvmvxzRawuIiESS\nhquGwe7dcN6tzzP8gqdY8dX3FAoiMuSo1eugsRE+e+0magtvoejGvzAmdUy0SxIR6XcaleTnHFx/\nUy3bTvsiD176Q81PEJEhS8Hgt/hux6sZc7my4FxuPuWmaJcjIhI1Cgbgqafg5+8/QP5UL7/8/M+i\nXY6ISFQN+WB480247SevkHD2T1k+548kJyRHuyQRkaiKeDCY2Wwz22Rmm81sQYCfZ5jZcjNba2br\nzWxupGtqt3UrXP6VYuxL1/H7q58hNyO3v95aRGTAiug8BjOLAzYD5wM7gdXANc65TR3WuRPIcM7d\naWZHAB8DWc65li7bCus8hupqmHH2PhqvPYtvzbqeb53xrbBtW0RkoOjNPIZI7zEUAFuccyXOuWZg\nGXBZl3UckO5/nA5Udg2FcGtqgi9e7kj8wtc469hj+PfT/z2SbyciElMiPY8hB/B0eO7FFxYd/RxY\nbmY7gTTg6kgW5Bx87WtQddSviM9dw28vfUcX1BER6WAgTHD7HLDGOTfLzCYBr5jZVOdcXdcV7777\n7gOPCwsLKSwsDPnN7r8f3ip5mz2z7+Gdq98mNSm195WLiAwwRUVFFBUV9WkbkT7GcDpwt3Nutv/5\nQsA55+7vsM5LwA+dc//nf/4qsMA5916XbfX5GMNzz8E3vluGu/k0Hv3CI1w8+eI+bU9EZKAbiMcY\nVgNHm1memSUB1wDLu6xTAlwAYGZZwBRge9gLWQ1f+3oTWfOv5NaCeQoFEZEeRPzsqmY2G/gpvhB6\n1Dn3IzO7Bd+ewyNmdiTwOHCk/yU/dM79LsB2+rTHcPPNsHHifDInFvPCNS8QZ0N+CoeIDAE67fYh\nTJ/ze3ad+D0++vfVjBw2MoyViYgMXAOxK2nAKE56iXnHLVQoiIgcxpAJhrp4D1Pz8qJdhojIgDck\ngqG2FtpSvRyXo1NeiIgczpAIBq/XQYaXCSPGR7sUEZEBb0gEw4fFnxLvhmsym4hIEIZGMHi8pLWp\nG0lEJBhDIhg2V3gYnaBuJBGRYAyJYCip8nBkqoJBRCQYQyIYyhq85GUqGEREgjEkgqGqxcPkLB1j\nEBEJxpAIhrp4DydO0B6DiEgwBn0wNDRAS4qHE8YrGEREgjHog8HjbYOMUnJH5ES7FBGRmDDog2HD\nJ7tJaE0nJTEl2qWIiMSEwR8MOzykt6kbSUQkWIM+GLZUeBmVqBFJIiLBGvTBUFLt4cgU7TGIiARr\n0AdDWYOHfE1uExEJ2qAPhqoWL5OzFAwiIsEa9MFQG+fhhAk6xiAiEqxBHQyNjb7JbZr1LCISvEEd\nDDs8rZC+k/EjNblNRCRYgzoYNnyyi8TWkQxLGBbtUkREYsbgDgaPhzRNbhMRCcmgDgZduU1EJHSD\nOhh2VHvJTtGIJBGRUAzqYChr8OjKbSIiIRrUwVDZ7GGKJreJiIRkUAdDXZyXEzSHQUQkJIM2GJqa\noDnFw9Q8HWMQEQnFoA0Gj7cV0sqZkKnJbSIioRi0wfDB9nISW0aTFJ8U7VJERGLKoA0G3+Q2dSOJ\niIRq0AaDJreJiPTOoA2GYl25TUSkVyIeDGY228w2mdlmM1vQwzqFZrbGzDaY2T/C8b7l9V7yMtWV\nJCISqoRIbtzM4oCfA+cDO4HVZvaCc25Th3VGAL8APuucKzWzI8Lx3pUtHiZnFYRjUyIiQ0qk9xgK\ngC3OuRLnXDOwDLisyzrXAn90zpUCOOc+Dccb18V5NLlNRKQXIh0MOYCnw3Ovf1lHU4BRZvYPM1tt\nZtf39U2bm6F5uJdp+QoGEZFQRbQrKUgJwMnALCAVeMfM3nHObe3tBr07WyB1FxMyjwxXjSIiQ0ak\ng6EUmNDhea5/WUde4FPnXCPQaGZvANOAbsFw9913H3hcWFhIYWFhwDdds3Unic1jSIxP7EvtIiIx\np6ioiKKioj5tw5xz4akm0MbN4oGP8R18LgNWAV92zm3ssM6xwEPAbCAZWAlc7Zz7qMu2XLC1Lnn8\nbR7ceDuV978blt9DRCRWmRnOOQvlNUEdYzCzP5nZJf5RRkFzzrUCtwEvAx8Cy5xzG83sFjOb519n\nE7AC+AB4F3ikayiEanOFh1EJGqoqItIbQe0xmNkFwI3A6cAfgMeccx9HuLauNQS9x3DuwqWQXsob\n3/vvCFclIjKwRWyPwTn3d+fcHHwHiYuBv5vZ22Z2o5kNuI58XblNRKT3gu4aMrPRwFzgq8Aa4Kf4\nguKViFTWB1UtXl25TUSkl4IalWRmzwPHAP8DXOqcK/P/6Fkzey9SxfVWXZyH48frGIOISG8EO1z1\nZ865gOcwcs6dGsZ6+qy1FZqGe5h+lPYYRER6I9iupM+Y2cj2J2aWaWb/FqGa+mRHaRMMryRvlCa3\niYj0RrDBcLNzbk/7E+dcNXBzZErqm7XbdpLYlEV8XHy0SxERiUnBBkO8mR0Y7uSfuDYgr5m5weMh\nvU3dSCIivRXsMYa/4TvQ/Gv/81v8ywacLRVeRunKbSIivRZsMCzAFwa3+p+/Avw2IhX1UUm1h+wU\njUgSEemtoILBOdcG/Mp/G9DKGjzMyJkU7TJERGJWsOdKmmxmz5nZR2a2vf0W6eJ6o7LZo8ltIiJ9\nEOzB58fw7S20AOcBTwJPRaqovqiL8+rKbSIifRBsMAx3zr2K76R7Jc65u4FLIldW77S1QdMwD9Mm\n6hiDiEhvBXvweb//lNtbzOw2fBfbSYtcWb3jKdsPw6vJG50V7VJERGJWsHsM3wRSgG8ApwDXAf8a\nqaJ6a822UpL2j9PkNhGRPjjsHoN/MtvVzrn/AOrwXZdhQNqww0Nqm7qRRET64rB7DP6rsJ3dD7X0\n2ZYKD6M1uU1EpE+CPcawxsyW47t6W337QufcnyJSVS8VV3k5MkXBICLSF8EGwzCgEpjVYZkDBlQw\nlDV4mHHkMdEuQ0QkpgU783nAHlfoqLLZw+SsC6JdhohITAv2Cm6P4dtD6MQ595WwV9QHdXFeThiv\nriQRkb4ItivppQ6PhwFfBHaGv5zec8535baTJikYRET6ItiupD92fG5mvwPeikhFveQp3wfJNeQd\nMSbapYiIxLRgJ7h1NRkYG85C+ur9LV6SGnOIs97+SiIiAsEfY6il8zGGcnzXaBgwNni8pLWqG0lE\npK+C7UpKj3QhfbWlwsOoRM16FhHpq2Cvx/BFMxvR4flIM/tC5MoKXXG1R5PbRETCINgO+cXOub3t\nT5xze4DFkSmpd8rrveRlKhhERPoq2GAItF6wQ137RWWLh8lj1ZUkItJXwQbDe2b2EzOb5L/9BPhn\nJAsLVW2cR1duExEJg2CDYT7QBDwLLAMaga9HqqhQOQdNw7ya3CYiEgbBjkqqBxZGuJZe81Y0QGID\n+WOOiHYpIiIxL9hRSa+Y2cgOzzPNbEXkygrN+1s9JDXmYGbRLkVEJOYF25V0hH8kEgDOuWoG0Mzn\nDR6PJreJiIRJsMHQZmYT2p+YWT4BzrYaLZvLvYzSldtERMIi2CGn3wPeMrPXAQPOAeZFrKoQlVR7\nODJVQ1VFRMIhqD0G59zfgFOBj4HfAd8G9gXzWjObbWabzGyzmfV4fiUzO83Mms3s8mC221FZg0eT\n20REwiTYk+h9FfgmkAusBU4H3qHzpT4DvS4O+DlwPr7rN6w2sxecc5sCrPcjoFcHtCubvUzO+pfe\nvFRERLoI9hjDN4HTgBLn3HnAScCeQ78EgAJgi3OuxDnXjG8OxGUB1psPPAfsCrKeTmrjPBw/Xl1J\nIiLhEGwwNDrnGgHMLNn/jf+YIF6XA3g6PPf6lx1gZuOALzjnfoXv+EVIfJPbPJyiyW0iImER7MFn\nr38ew5+BV8ysGigJUw0P0vnaDiGFg3d3LcQ3kTd2VJjKEREZ2oKd+fxF/8O7zewfwAjgb0G8tBSY\n0OF5rn9ZR6cCy8w3O+0I4CIza3bOLe+6sbvvvvvA48LCQgoLC1mz1UvSvvGa3CYiAhQVFVFUVNSn\nbZhzkZuOYGbx+EYynQ+UAauALzvnNvaw/mPAi865PwX4mQtU633LXuYnK++n8r9fDWvtIiKDgZnh\nnAvpm3NET53tnGs1s9uAl/Edz3jUObfRzG7x/dg90vUlob7H5nKPJreJiIRRxK+p4J8DcUyXZb/u\nYd2vhLr9kmovR6YqGEREwiXYUUkDVlmDhwkjNVRVRCRcYj4YKps9TMnSHoOISLjEfDDUxXk5fryC\nQUQkXGI+GPYP83DK0epKEhEJl5gOBs/uvWBtTBg78vAri4hIUGI6GNont8XFaXKbiEi4xHQwrN/h\nIa1NxxdERMIppoNhc4WHUQk6viAiEk4xHQwl1V6yU7THICISTjEdDGX1HvJ15TYRkbCK6WCobPEw\nOUtdSSIi4RTTwVAX59HkNhGRMIvZYHDOsT/ZyylHKxhERMIpZoPBW7kH2uLJy86IdikiIoNKzAbD\nmm0ekhpz0YXbRETCK2aDYcMOL6ma3CYiEnYxGwybKzyM1pXbRETCLmaDobjKQ3aKhqqKiIRbzAZD\nWb2XvJHaYxARCbeYDYbKFl25TUQkEmI2GGrjPBw/Xl1JIiLhFpPB4JyjKdnLyZrcJiISdjEZDKXV\nVdCSTP64tGiXIiIy6MRkMLy/1UNS43hNbhMRiYCYDIYNOzyktur4gohIJMRkMGyu8Gpym4hIhMRk\nMPgmtykYREQiISaDoazBQ16mupJERCIhJoOhstmryW0iIhESk8GgK7eJiEROzAVD+5XbTpqkriQR\nkUiIuWDwVu+GplTyc1KiXYqIyKAUc8GwZpuXxH3jiYu5ykVEYkPMNa8bdnhI05XbREQiJuaCYXOF\nh1EJOr4gIhIpMRcMJdVejtTkNhGRiIl4MJjZbDPbZGabzWxBgJ9fa2br/Le3zOzEQ21vZ72HvEwF\ng4hIpCREcuNmFgf8HDgf2AmsNrMXnHObOqy2HTjXObfXzGYDvwFO72mblc0eJo9VV5LIUJSfn09J\nSUm0yxiQ8vLyKC4uDsu2IhoMQAGwxTlXAmBmy4DLgAPB4Jx7t8P67wI5h9pgrXk1uU1kiCopKcE5\nF+0yBiQL43UIIt2VlAN4Ojz3cuiG/6vA//b0wzbXRtOwUk1uExGJoEjvMQTNzM4DbgTO7mmdby+8\nA96K5wl+xKxZhRQWFvZTdSIisaGoqIiioqI+bcMiuVtmZqcDdzvnZvufLwScc+7+LutNBf4IzHbO\nbethW275e6u54vF5ND30fsRqFpGBy8zUldSDnj4b//KQ+pki3ZW0GjjazPLMLAm4BljecQUzm4Av\nFK7vKRTardfkNhGRiItoMDjnWoHbgJeBD4FlzrmNZnaLmc3zr7YIGAX80szWmNmqnra3udzDqHgF\ng4gMPrfeeiv33XdftMsAItyVFE5m5mbe9x+01I7mrR8ujHY5IhIFA7kraeLEiTz66KPMmjUrKu8f\nzq6kAXPwORhl9V5OHTk92mWIyADzySclLFr0OKWlbeTkxLFkyVwmTszr9230pLW1lfj4+LBsq184\n52LiBrjR3znLLX7sdSciQ5Ovyeps+/ZiN2nStx3UOXAO6tykSd9227cXB73dvm7j+uuvd3FxcW74\n8OEuPT3dPfDAA87M3KOPPuomTJjgZs6c6Zxz7sorr3TZ2dlu5MiRbubMme7DDz88sI25c+e6RYsW\nOeecKyoqcrm5ue7HP/6xGzt2rBs3bpx77LHHDllDoM+mw/KQ2tuYOldSbZyHE8ZrDoOIHLRo0eNs\n23YPkOpfksq2bfewaNHj/baNJ598kgkTJvCXv/yFmpoarrrqKgDeeOMNNm3axIoVKwC4+OKL2bZt\nG7t27eLkk09mzpw5PW6zvLyc2tpadu7cyW9/+1u+/vWvs3fv3qB/p76IqWBoSipj+qRDTowWkSGm\ntLSNgw2ZK8sfAAAKj0lEQVR6u1SefroNM4K6Pf104G3s3NkWUi2uQx+/mXHPPfcwfPhwkpOTAZg7\ndy4pKSkkJiZy1113sW7dOmprawNuKykpiUWLFhEfH89FF11EWloaH3/8cUj19FZMBQP7RpGXkxzt\nKkRkAMnJiQPquyytZ86cOJy/Y+hwtzlzAm9j3Li+NZG5uQd7ONra2li4cCFHH300I0eOZOLEiZgZ\nn376acDXjh49mrgOVyRLSUmhrq6uT/UEK6aCIXFfLomJ0a5CRAaSJUvmMmnSYg427PVMmrSYJUvm\n9us2Ap2rqOOyZ555hhdffJHXXnuNPXv2UFxc3PEY6oASU6OS0lo1h0FEOps4MY9XXpnPokVL2bmz\njXHj4liyZH5II4rCsY3s7Gy2b9/OrFmzAjb4tbW1JCcnk5mZSX19PXfeeWdYT3wXTjEVDKMSFAwi\n0t3EiXk89dTiqG5j4cKFzJ8/nzvuuIPvfe973Rr9G264gRUrVpCTk8Po0aNZsmQJv/71r4Pefn+G\nSExNcDvrjgd46/7vRLsUEYmSgTzBLdpi6VxJYZWXqaGqIiKRFlPBMHmsupJERCItpoLhBF25TUQk\n4mLqGMNpV51DyvA4pmRN4ZH7H4l2SSLSz3SMoWdD9iR6qz/zpu/BJ9GtQ0RkMIuprqR2+/Y1RrsE\nEZFBKyaDobi4PNoliIgMWjEZDPv3q49RRCRSYjIYkpMH5jRyEZHBILaC4bGzGbYsl1knnxHtSkRE\nOpk4cSKvvfZan7bxxBNPcM4554Spot6LqVFJc86+IKyX2xOR2DdvwTw2V2zutjyUYe3h2EY4OOcG\nxIn1YioY+nqSLBEZfDZXbOb1ia93/0EIw9r7uo0bbriBHTt2cOmllxIfH89dd93FOeecw+23385H\nH31Efn4+Dz74IDNnzgTg8ccfZ8mSJezevZsxY8bwgx/8gJNOOolbb72VlpYW0tPTSUxMpKqqKvhf\nIoxiqytJRGQAar+050svvURNTQ3XXnstl1xyCXfddRfV1dUsXbqUK664gsrKShoaGvjmN7/JihUr\nqKmp4e2332b69Okce+yxPPzww5xxxhnU1tZGLRQgxvYYRESC9Xrx69g9QXbLFAMT+/6e7TOPn3rq\nKS655BI+97nPAXD++edz6qmn8te//pUrrriC+Ph41q9fT25uLllZWWRlZfX9zcNIwSAig9LM/JkU\nLS4Kat3CTwp5nQBdSb1UUlLC73//e1588UXAFxgtLS3MmjWLlJQUnn32Wf7rv/6Lr3zlK5x99tks\nXbqUY445Jmzv31fqShIRCYOOB43Hjx/PDTfcQFVVFVVVVVRXV1NbW8sdd9wBwIUXXsjLL79MeXk5\nxxxzDPPmzeu2jWjSHoOIxLQpWVMCHiSekjWlX7fR8dKe1113HQUFBVxxxRVccMEFNDU1sXLlSiZP\nnkxCQgLvvvsuF1xwAcOGDSMtLY24ON939KysLLxeL83NzSRG8QL3MXV21VipVUQiYyCfXXX58uXM\nnz+f2tpavv/973POOefwne98h/Xr15OQkEBBQQG/+tWvSEhI4JprrmHdunWYGdOnT+eXv/wlxx57\nLM3NzVx++eW8/fbbxMfHs2vXrqDfP5xnV1UwiEjMGMjBEG1D9tKeIiISeQoGERHpRMEgIiKdKBhE\nRKQTBYOIiHSiYBARkU40wU1EYkZeXt6AmR080OTlhe9yBBGfx2Bms4EH8e2dPOqcuz/AOj8DLgLq\ngbnOubUB1tE8BhGREA24eQxmFgf8HPgccDzwZTM7tss6FwGTnHOTgVuAhyNZk/gUFRVFu4RBRZ9n\n+OizjL5IH2MoALY450qcc83AMuCyLutcBjwJ4JxbCYwws4F1DtpBSP/5wkufZ/jos4y+SAdDDuDp\n8NzrX3aodUoDrCMiIv1Eo5JERKSTiB58NrPTgbudc7P9zxcCruMBaDN7GPiHc+5Z//NNwEznXEWX\nbenIs4hIL4R68DnSw1VXA0ebWR5QBlwDfLnLOsuBrwPP+oNkT9dQgNB/MRER6Z2IBoNzrtXMbgNe\n5uBw1Y1mdovvx+4R59xfzexiM9uKb7jqjZGsSUREDi1mrscgIiL9IyYOPpvZbDPbZGabzWxBtOuJ\ndWZWbGbrzGyNma2Kdj2xxMweNbMKM/ugw7JMM3vZzD42sxVmNiKaNcaSHj7PxWbmNbP3/bfZ0awx\nVphZrpm9ZmYfmtl6M/uGf3nIf58DPhiCmSQnIWsDCp1zJznnCqJdTIx5DN/fYkcLgb87544BXgPu\n7PeqYlegzxPgJ865k/23v/V3UTGqBbjdOXc8cAbwdX9bGfLf54APBoKbJCehMWLj337Acc69BVR3\nWXwZ8IT/8RPAF/q1qBjWw+cJvr9RCYFzrrz9dELOuTpgI5BLL/4+Y6FxCGaSnITGAa+Y2Wozuzna\nxQwCY9tH0jnnyoGxUa5nMLjNzNaa2W/VNRc6M8sHpgPvAlmh/n3GQjBI+J3lnDsZuBjf7ubZ0S5o\nkNGIjr75JXCUc246UA78JMr1xBQzSwOeA77p33Po+vd42L/PWAiGUmBCh+e5/mXSS865Mv/9buB5\nfN110nsV7ef3MrNsYFeU64lpzrndHU6l/BvgtGjWE0vMLAFfKPyPc+4F/+KQ/z5jIRgOTJIzsyR8\nk+SWR7mmmGVmKf5vFJhZKvBZYEN0q4o5Ruc+8OXAXP/jfwVe6PoCOaROn6e/8Wp3Ofr7DMX/Az5y\nzv20w7KQ/z5jYh6Df7jaTzk4Se5HUS4pZpnZRHx7CQ7fBMen9XkGz8yeAQqB0UAFsBj4M/AHYDxQ\nAlzlnNsTrRpjSQ+f53n4+sfbgGLglkBnQ5DOzOws4A1gPb7/3w74LrAK+D0h/H3GRDCIiEj/iYWu\nJBER6UcKBhER6UTBICIinSgYRESkEwWDiIh0omAQEZFOFAwiEWJmM83sxWjXIRIqBYNIZGmikMQc\nBYMMeWY2x8xW+i8K8yszizOzWjP7iZltMLNXzGy0f93pZvaO/8yff2w/86eZTfKvt9bM3vPPMAdI\nN7M/mNlGM/ufDu/5I/+215rZA1H4tUV6pGCQIc1/IZOrgTP9Z5xtA+YAKcAq59wJ+E4zsNj/kieA\n7/jP/Lmhw/KngYf8y88EyvzLpwPfAD4DTDKzM81sFPAF59wJ/vV/EOnfUyQUCgYZ6s4HTgZWm9ka\nYBYwEV9A/N6/zlPA2WaWAYzwX1wGfCFxrv+khDnOueUAzrkm51yjf51Vzrky/9lC1wL5wF5gn/9a\nA18E9kX8txQJgYJBhjoDnvBfQvIk59xxzrl7A6znOqwfiv0dHrcCCc65VnynOn8O+DygS1fKgKJg\nkKHuVeBLZjYGDlw4fQIQD3zJv84c4C3nXA1Q5T+LJcD1wOv+i6F4zOwy/zaSzGx4T29oZinASP+1\njG8HpkbiFxPprYRoFyASTc65jWb2feBlM4sDmoDbgHqgwMwW4Tsd9NX+l/wr8Gt/w78duNG//Hrg\nETO717+NKwO9nf8+A3jBzIb5n38rzL+WSJ/otNsiAZhZrXMuPdp1iESDupJEAtM3JhmytMcgIiKd\naI9BREQ6UTCIiEgnCgYREelEwSAiIp0oGEREpBMFg4iIdPL/Ac1qAOPlvcVwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ad6a668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from ch07.simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "%matplotlib inline \n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEcCAYAAAC4WdEMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAHCF4gMAuELxAQBcofgAAK5QfAAAV/4Pnl+57upzWT0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1065d2780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEcCAYAAAC4WdEMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns3/dKD0g1oMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c801f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "%matplotlib inline \n",
    "network = SimpleConvNet()\n",
    "# ランダム初期化後の重み\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 学習後の重み\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
